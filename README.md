# VAE Hybrid Music Clustering (Easy + Medium + Hard)

**Student:** Tahsin (ID: 1000054859)  
**Repo:** `vae-hybrid-music-clustering-Tahsin-ID-1000054859`  
**GitHub:** https://github.com/tahsin12zaman/vae-hybrid-music-clustering-Tahsin-ID-1000054859

This project performs unsupervised clustering on a small multilingual music dataset using VAE-based representations and hybrid multi-modal features (**audio + lyrics + genre**).

---

## What’s Implemented

### Easy Task
- Basic **VAE** embeddings → **K-Means**
- Baseline: **PCA + K-Means**
- Visualization: **UMAP**

### Medium Task
- **Conv-VAE** on **log-mel spectrograms**
- Hybrid representation: **audio latent + lyrics embeddings**
- Clustering: **K-Means, Agglomerative (Ward), DBSCAN**
- Metrics: **Silhouette, Davies–Bouldin, ARI** (language used as partial label)

### Hard Task
- **Beta-VAE** (and AE baseline via `beta=0`)
- Multi-modal clustering: **audio + lyrics** (and optional **genre one-hot** feature)
- Metrics: **Silhouette, ARI, NMI, Cluster Purity**
- Visualizations: **t-SNE latent plots**, **cluster distributions**, **reconstructions**

---

## Repository Structure (high level)

- `data/`
  - `lyrics/lyrics.csv` (columns: filename, language, genre, lyrics)
  - `genre_map.csv`, `meta_with_genre.csv`
  - *(raw audio is intentionally not tracked in git)*
- `src/`
  - `easy_task_scripts/`
  - `medium_task_scripts/`
  - `hard_task_scripts/`
- `results_easy/` : Easy task outputs (metrics + plots)
- `results_medium/` : Medium task outputs (features/embeddings + clustering metrics)
- `results_hard/` : Hard task outputs (clustering metrics + plots)
- `REPORT.pdf` : NeurIPS-style report (placed in repo root for submission)

---

## Setup

Recommended (Ubuntu / Python 3.8+):

```bash
# system dependency (Ubuntu)
sudo apt-get install -y ffmpeg

# python env
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# optional: only if you want ASR/Whisper to auto-generate lyrics
pip install -U openai-whisper

```

Run the steps in order; later scripts read outputs generated by earlier steps.
## How to Run (end-to-end)

### Easy Task (VAE + KMeans, PCA baseline, UMAP)

```bash
python3 src/easy_task_scripts/extract_features.py
python3 src/easy_task_scripts/train_vae.py
python3 src/easy_task_scripts/cluster_eval.py
python3 src/easy_task_scripts/visualize.py
```
### Outputs


- results_easy/metrics/metrics.csv

- results_easy/plots/

### Medium Task (Conv-VAE + lyrics embeddings + clustering + metrics)

> Assumes your raw audio is available locally under `data/audio/...` (raw audio is not tracked in git).

1) **Ensure `data/lyrics/lyrics.csv` has lyrics text**  
If lyrics are missing, you can generate them via ASR (Whisper) from your local audio:

```bash
python3 src/medium_task_scripts/build_lyrics_csv.py \
  --in_csv data/lyrics/lyrics.csv \
  --audio_dir data/audio \
  --model small \
  --force
```
2)Build lyrics embeddings
```
python3 src/medium_task_scripts/lyrics_embed.py
```
3)Extract audio features (log-mel)
```
python3 src/medium_task_scripts/extract_audio_features_medium_task.py \
  --audio_root data/audio \
  --seconds 30 \
  --overwrite
```
4)Train Conv-VAE on audio
    (Example config; use your preferred settings.)
```
python3 src/medium_task_scripts/train_conv_vae_audio_medium_task.py \
  --epochs 400 \
  --latent_dim 16 \
  --beta 0.1 \
  --kl_warmup 100
```
5)Cluster + evaluate (KMeans / Agglomerative / DBSCAN; Silhouette/DB/ARI)
```
python3 src/medium_task_scripts/cluster_medium_task.py
```
### Outputs

- Metrics: results_medium/clustering/metrics_medium.csv (+ .json)

- Audio latent: results_medium/conv_vae_audio/z_audio_mu.npy

- Lyrics embedding: results_medium/embeddings/Z_lyr.npy

### Hard Task (Beta-VAE + multimodal clustering + visualizations)

> Assumes your raw audio is available locally under `data/audio/...` (raw audio is not tracked in git).

1) **Add genre metadata** (merge `data/genre_map.csv` into `data/lyrics/lyrics.csv`)

```bash
python3 src/hard_task_scripts/build_meta_with_genre.py \
  --genre_csv data/genre_map.csv \
  --overwrite_lyrics
```

2)**Train Autoencoder baseline**  (same Conv trainer with beta=0)
```
python3 src/medium_task_scripts/train_conv_vae_audio_medium_task.py \
  --beta 0 \
  --kl_warmup 0 \
  --epochs 400 \
  --latent_dim 16 \
  --out_dir results_hard/ae_audio
```

3)Run hard-task clustering + evaluation
(Computes ARI/NMI/Purity for language + genre, and compares PCA/AE/Beta-VAE + spectral baselines.)
```
python3 src/hard_task_scripts/cluster_hard_task.py \
  --ae_latent_path results_hard/ae_audio/z_audio_mu.npy
```

Optional: **Include genre as a feature**  (genre one-hot) in multimodal clustering:
```
python3 src/hard_task_scripts/cluster_hard_task.py \
  --ae_latent_path results_hard/ae_audio/z_audio_mu.npy \
  --include_genre_feature \
  --gamma 1.0

```
4)**Generate plots** (t-SNE latent plots + cluster distributions + reconstructions)
```
python3 src/hard_task_scripts/viz_hard_task.py
```

### Outputs

- Metrics: results_hard/clustering/metrics_hard.csv (+ .json)

- Plots: results_hard/plots/

- Latent t-SNE plots (by language / genre / cluster)

- Cluster vs label distribution plots

- Reconstruction examples (orig_logmel_sample0.png, recon_AE_sample0.png, recon_betaVAE_sample0.png)
